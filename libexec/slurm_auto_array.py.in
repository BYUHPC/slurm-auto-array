#!/usr/bin/env python3





"""
This script is meant to help users submit job arrays easily. Much like GNU parallel, it takes a newline-delimited list
of arguments and optionally runs a user-supplied command on each argument using a job array. task.sh in the utils
directory is the batch script that is used to run the commands.

The job array consists of up to 1000 (by default) tasks, each of which will run one or more instances of the command
(hence "work units"). The amount of work units to run per array task is chosen to try to go easy on the scheduler and
maximize throughput, although the desired array task size can be changed by the user. By default each array task will
use up to 4 CPUs, 1 GPU, and 16G memory for up to 6 hours. If the work units cannot be squeezed into such small tasks,
or cannot fit into such small tasks without exceeding 1000 total tasks, the time limit is increased to up to 1 day by
default; if that is still insufficient the default array task limits are discarded and "fatter" tasks are submitted.

In order to pass stdin to task.sh, a temporary file (arg_file) is created in the directory specified by
SAA_ARG_FILE_DIR; it will be automatically deleted on job completion.
"""





import re, os, sys, shutil, tempfile, subprocess, argparse, math, shlex, logging, time





# USAGE ################################################################################################################

description = """slurm-auto-array automates the submission of sbatch job arrays. It takes a newline-delimited (by
                 default) set of commands on stdin and submits a job array that runs an optional user-specified command
                 on each argument. For example, one could run 'myprog.sh' on every file ending with '.dat' in the
                 current directory using 'ls *.dat | slurm-auto-array -- myprog.sh'. Multiple instances of the command
                 may run per array task (e.g. 24 instances of your command could be run in 3 array tasks with 8
                 instances each); you can control the size of the array tasks with the '--per-task-*' flags. If you wish
                 to specify part of the command from the command line, for example to run a single command with fixed
                 arguments on several files, add '-- command [arg1 [arg2 ...]]' to the end of the slurm-auto-array
                 invocation; note that the '--' is *not* optional.

                 In addition to the listed arguments, any sbatch arguments that won't interfere with slurm-auto-array's
                 operation can be supplied; see the man page for a list."""

epilog = "See the man page or https://rc.byu.edu/wiki/?id=slurm-auto-array for more details and examples."

version = "SAA_VERSION_STRING" # this gets substituted by make

########################################################################################################################





# FORMAT CONVERSION FUNCTIONS ##########################################################################################

# Convert a time string (specification in `man sbatch`) or integer to minutes, ignoring seconds (since so does Slurm)
def time_to_mins(time):
    time_str = str(time)
    if re.match(r"^\d+$", time_str):               # minutes
        return int(time_str)
    elif re.match(r"^\d+:\d+$", time_str):         # minutes:seconds
        minutes, seconds = time_str.split(":")
        return int(minutes)
    elif re.match(r"^\d+:\d+:\d+$", time_str):     # hours:minutes:seconds
        hours, minutes, seconds = time_str.split(":")
        return 60 * int(hours) + int(minutes)
    elif re.match(r"^\d+-\d+$", time_str):         # days-hours
        days, hours = time_str.split("-")
        return 24 * 60 * int(days) + 60 * int(hours)
    elif re.match(r"^\d+-\d+:\d+$", time_str):     # days-hours:minutes
        days, hours_minutes = time_str.split("-")
        hours, minutes = hours_minutes_seconds.split(":")
        return 24 * 60 * int(days) + 60 * int(hours) + int(minutes)
    elif re.match(r"^\d+-\d+:\d+:\d+$", time_str): # days-hours:minutes:seconds
        days, hours_minutes_seconds = time_str.split("-")
        hours, minutes, seconds = hours_minutes_seconds.split(":")
        return 24 * 60 * int(days) + 60 * int(hours) + int(minutes)
    else:
        raise ValueError(f"Time specification {time_str} invalid")
    # TODO: return once, use `positive_int`

# Convert a memory string (<number>{K|M|G|T}) to megabytes
def mem_to_mb(mem):
    mem_str = str(mem)
    for suffix, multiplier in [('K', 1024**-1), ('M', 1), ('', 1), ('G', 1024), ('T', 1024**2)]:
        if re.match(f'^\d+{suffix}$', mem_str, re.IGNORECASE):
            return positive_int(re.sub(r'[a-zA-Z]', '', mem_str)) * multiplier
    raise ValueError(f'Memory specification {mem_str} invalid')

# Positive int
def positive_int(arg):
    iarg = int(arg)
    if iarg <= 0:
        raise ValueError(f'Expected a positive integer; got {arg}')
    return iarg

# Non-negative int
def nonnegative_int(arg):
    iarg = int(arg)
    if iarg < 0:
        raise ValueError(f'Expected a nonnegative integer; got {arg}')
    return iarg

########################################################################################################################





# ALLOCATION SIZE ######################################################################################################

# A class that contains the size (in CPUs, GPUs, memory, and time) of an allocation
class AllocationSize:
    def __init__(self, *args):
        # Construct from allocation size string
        cgmt = args[0].split(",") if len(args) == 1 else args
        self.cpus = positive_int(cgmt[0])
        self.gpus = nonnegative_int(cgmt[1])
        self.mem  = mem_to_mb(cgmt[2])
        self.time = time_to_mins(cgmt[3])
        # TODO: error handling

    def __repr__(self):
        return f"{self.cpus},{self.gpus},{self.mem}M,{self.time}"



def array_task_size_and_count(work_unit_count, work_unit_size, requested_array_task_size):
    # How many array tasks of the desired size are needed to fit all the work units?
    parallel_units = math.floor(min(requested_array_task_size.cpus / work_unit_size.cpus,
                                    requested_array_task_size.gpus / work_unit_size.gpus if work_unit_size.gpus
                                                                                         else math.inf,
                                    requested_array_task_size.mem  / work_unit_size.mem)) or 1
    batches_per_task = math.floor(requested_array_task_size.time / work_unit_size.time) or 1
    requested_array_task_count = math.ceil(work_unit_count / parallel_units / batches_per_task)

    # How long and wide should each task actually be? Increase time, then width
    work_units_per_task = math.floor(work_unit_count / requested_array_task_count) or 1
    length = min(batches_per_task, work_units_per_task)
    width = math.ceil(work_units_per_task / length)

    # Actual size of array tasks
    array_task_size = AllocationSize(work_unit_size.cpus * width,
                                     work_unit_size.gpus * width,
                                     work_unit_size.mem  * width,
                                     work_unit_size.time * length)

    # Minimize array task count--this way there will be at most one underutilized task
    array_task_count = math.ceil(work_unit_count / width / length)

    return array_task_size, array_task_count

########################################################################################################################





# PARSING ##############################################################################################################

# Parse a file for #SBATCH/#SAA flags, and return the amended command
def file_sbatch_flags(filename):
    """
    If the name of a file in the working directory shadows a command in PATH, the file will be used, just as with sbatch
    """
    if not filename or (not os.path.isfile(filename) and not shutil.which(filename)):
        return []
    flags = []
    with open(filename if os.path.isfile(filename) else shutil.which(filename), "r") as f:
        for line in (line.strip() for line in f):
            if line and line[0] != "#":
                return flags
            elif line and (line[0:8] == "#SBATCH " or line[0:5] == "#SAA "):
                for flag in shlex.split(line)[1:]:
                    if flag and flag[0] == "#": break
                    flags.append(flag)
    return flags



# Read in configuration from the environment and/or config file
def get_config():
    default_arg_file_dir = os.path.join(os.path.expanduser("~"), ".local", "share", "saa-arg-files")
    user_config = os.path.join(os.path.expanduser("~"), ".config", "slurm-auto-array.conf")
    global_config = os.path.join("etc", "slurm-auto-array.conf")
    config_file = os.getenv("SAA_CONFIG_FILE", user_config   if os.path.isfile(user_config)   else
                                               global_config if os.path.isfile(global_config) else None)

    config_dict = {}
    if config_file:
        with open(config_file) as f:
            for line in f.readlines():
                key, value = line.split(" ", 1)
                config_dict[key] = value

    def mkdir_if_possible(dirname):
        try:
            os.makedirs(dirname, exist_ok=True)
            return dirname
        except OSError:
            raise ValueError(f"Couldn't create directory {dirname}")

    def get_value(f, name, default):
        try:
            return f(config_dict[name] if name in config_dict else os.getenv(name, default))
        except ValueError:
            logging.warning(f"{name} set to invalid value; using default of {default}")
            return f(default)

    return (get_value(tf, name, default)
            for name, default, tf in (("SAA_ARG_FILE_DIR",            default_arg_file_dir,      mkdir_if_possible),
                                      ("SAA_MAX_ARRAY_TASKS",         "1000",                    positive_int),
                                      ("SAA_DEFAULT_WORK_UNIT_SIZE",  "1,0,2G,1:00:00",          AllocationSize),
                                      ("SAA_DEFAULT_ARRAY_TASK_SIZE", "4,1,16G,6:00:00",         AllocationSize),
                                      ("SAA_MAX_ARRAY_TASK_SIZE",     "1024,64,16T,30-00:00:00", AllocationSize)))



# Parse arguments, including filtering out forbidden sbatch arguments and parsing the command for #SBATCH/#SAA flags
# Single-letter arguments that sbatch doesn't use: E, f, g, I, j, K, l, P, r, R, T, u, U, X, y, Y, z, Z
# Already in use by us:                                              ^           ^     ^
def parse(default_work_unit_size, default_array_task_size, args=sys.argv[1:]):
    # Cut off everything after "--"
    command = []
    try:
        split = args.index("--")
        command = args[split+1:]
        args = args[:split]
    except ValueError:
        pass

    # Add #SBATCH/#SAA files from the input file, if there is one
    parsed_sbatch_file = True
    try:
        args = file_sbatch_flags(command[0] if command else '') + args
    except UnicodeDecodeError:
        parsed_sbatch_file = False


    # Make sure we don't have any no-no arguments
    for arg in ["-a", "--array",
                "--cpus-per-gpu",
                "--cpus-per-task",
                "--gpus-per-node",
                "--gpus-per-socket",
                "--gpus-per-task",
                "--ntasks-per-core",
                "--ntasks-per-node",
                "--ntasks-per-socket",
                "--wrap"]:
        if any(filter(re.compile(f"^{arg}(=.*)?$").match, args)):
            raise ValueError(f"sbatch argument '{arg}' disallowed")

    # Parse main arguments
    parser = argparse.ArgumentParser(description=description, epilog=epilog,
                                     usage='%(prog)s [options] [-- command [args ...]]')
    parser.add_argument("-V", "--version", action="version", version=version)
    parser.add_argument("--delimiter", metavar="D", default="\\n",
                        help="string or regular expression separating work unit command lines; newline by default")
    parser.add_argument("-n", "--ntasks", metavar="N", type=positive_int,
                        help=f"number of CPUs required by each unit of work ({default_work_unit_size.cpus} by default)")
    parser.add_argument("-G", "--gpus", metavar="N", type=nonnegative_int,
                        help=f"number of GPUs required by each unit of work ({default_work_unit_size.gpus} by default)")
    memgrp = parser.add_mutually_exclusive_group()
    memgrp.add_argument("-m", "--mem", metavar="N{K|M|G|T}", help="memory required by each unit of work")
    memgrp.add_argument("--mem-per-cpu", metavar="N{K|M|G|T}",
                        help=f"memory required by each CPU ({default_work_unit_size.mem}M by default)")
    memgrp.add_argument("--mem-per-gpu", metavar="N{K|M|G}", help="memory required by each GPU")
    parser.add_argument("-t",  "--time", metavar="D-HH:MM:SS",
                        help=f"time required by each work unit ({default_work_unit_size.time} mins by default); see " +
                              "sbatch(1) for formatting")
    parser.add_argument("-U", "--work-unit-size", metavar="CPUs,GPUs,mem,time", type=AllocationSize,
                        default=default_work_unit_size,
                        help=f"Allocation size of each work unit, with respective formats those of -n, -G, -m, and -t" +
                             f" ({default_work_unit_size} by default); this argument has lower precedence than the " +
                             "individual arguments it represents") 
    parser.add_argument("-T", "--array-task-size", metavar="CPUs,GPUs,mem,time", type=AllocationSize,
                        default=default_array_task_size,
                        help=f"Maximum desired allocation size of each array task, with the same format as -U " +
                             f"({default_array_task_size} by default); can be exceeded if it doesn't allow the job to" +
                             " fit within the maximum job array count")
    parser.add_argument("-l", "--logfile", metavar="output.log", default="slurm-auto-array-%A.log",
                        help="slurm-auto-array log file. Setting to /dev/null will suppress all output, including " +
                             "output and error files (slurm-auto-array-%%A.log by default)")
    parser.add_argument("-o", "--output", metavar="output.out", default="slurm-auto-array-%A_%a.out",
                        help="File (with optional formatting) to which to write stdout for each array task; only a " +
                             r"subset of sbatch's formatting options are supported, namely %%a, %%A, %%N, %%u, and " +
                             r"%%x; see sbatch(1) (slurm-auto-array-%%A_%%a.out by default); you can also specify %%0" +
                             r" to replace the command being run, %%1 to replace its first argument, %%2 to replace " +
                             "the second, etc.")
    parser.add_argument("-e", "--error", metavar="output.err", default=None,
                        help="Analogous to output, but for stderr (stderr goes to output file by default)")
    parser.add_argument("--open-mode", metavar="{append|truncate}", default="truncate",
                        help="Open log, output, and error files in the specified mode; see sbatch(1).")
    parser.add_argument("-v", "--verbose", action="store_true", help="print verbose output")
    parser.add_argument("--dry-run", action="store_true", help="don't submit, but print what would have been done")

    # Parse
    args, slurm_args = parser.parse_known_args(args)
    if args.error is None:
        args.error = args.output

    # Setting up the most basic possible logger takes 6 lines for some reason
    logging_handler = logging.StreamHandler()
    logging_handler.setFormatter(logging.Formatter("%(levelname)s: %(message)s"))
    logger = logging.getLogger("slurm_auto_array_logger")
    logger.addHandler(logging_handler)
    logger.setLevel(logging.INFO if args.verbose else logging.WARN)
    logger.propagate = False

    # Return all the information we've found
    if not parsed_sbatch_file:
        logger.info(f"File '{command[0]}' cannot be read as text and will not be parsed for '#SBATCH' flags")
    return args, slurm_args, command, logger

########################################################################################################################





# MAIN #################################################################################################################

def main(cmd_line_args=sys.argv[1:]):
    # Parse
    arg_file_dir, max_array_tasks, default_work_unit_size, default_array_task_size, max_array_task_size = get_config()
    args, sbatch_args, command, logger = parse(default_work_unit_size, default_array_task_size, cmd_line_args)

    # Exit if GNU Parallel hasn't been set up
    if not os.path.exists(os.path.expanduser("~/.parallel/will-cite")):
        logger.error(f"You must run `parallel --citation` before using {sys.argv[0]}; exiting")
        sys.exit(1)
    if not os.path.exists(os.path.expanduser("~/.parallel/ignored_vars")):
        logger.error(f"You must run `parallel --record-env` in a clean environment before using {sys.argv[0]}; exiting")
        sys.exit(1)

    # Get arguments from stdin to figure out how many work units there are
    delimiter_literal = args.delimiter.encode("ascii", "ignore").decode("unicode_escape")
    work_unit_args = [line for line in sys.stdin.read().split(delimiter_literal) if line]
    work_unit_count = len(work_unit_args)
    if not work_unit_count:
        print("No arguments provided--exiting")
        sys.exit(1)

    # Determine size of work units
    work_unit_size = args.work_unit_size
    if args.ntasks:        work_unit_size.cpus = args.ntasks
    if args.gpus:          work_unit_size.gpus = args.gpus
    if args.mem:           work_unit_size.mem  = mem_to_mb(args.mem)
    elif args.mem_per_gpu: work_unit_size.mem  = mem_to_mb(args.mem_per_gpu) * args.gpus
    elif args.mem_per_cpu: work_unit_size.mem  = mem_to_mb(args.mem_per_cpu) * args.ntasks
    if args.time:          work_unit_size.time = time_to_mins(args.time)
    if not work_unit_size.mem:
        raise ValueError("Flawed or missing memory specification")

    # Determine size of array tasks
    requested_array_task_size = AllocationSize(min(args.array_task_size.cpus, max_array_task_size.cpus),
                                               min(args.array_task_size.gpus, max_array_task_size.gpus),
                                               min(args.array_task_size.mem,  max_array_task_size.mem),
                                               min(args.array_task_size.time, max_array_task_size.time))
    array_task_size, array_task_count = array_task_size_and_count(work_unit_count, args.work_unit_size,
                                                                  requested_array_task_size)
    # Increase width if size isn't yet sufficient
    if array_task_count > max_array_tasks:
        requested_array_task_size.cpus = max_array_task_size.cpus
        requested_array_task_size.gpus = max_array_task_size.gpus
        requested_array_task_size.cpus = max_array_task_size.mem
        array_task_size, array_task_count = array_task_size_and_count(work_unit_count, args.work_unit_size,
                                                                      requested_array_task_size)
    # Increase time if size still isn't sufficient
    if array_task_count > max_array_tasks:
        array_task_size, array_task_count = array_task_size_and_count(work_unit_count, args.work_unit_size,
                                                                      max_array_task_size)

    # Let the user know what's been parsed if verbose
    logger.info( "Finished parsing arguments; final values:")
    if command:
        logger.info( "--- command to be run ---")
        logger.info(f"    {' '.join(command)}")
    logger.info(f"--- configuration ---")
    logger.info(f"    default work unit size: {default_work_unit_size}")
    logger.info(f"    default array task size: {default_array_task_size}")
    logger.info(f"    max array task count: {max_array_tasks}")
    logger.info(f"    max array task size: {max_array_task_size}")
    logger.info( "--- per work unit ---")
    logger.info(f"    CPUs:        {work_unit_size.cpus}")
    logger.info(f"    GPUs:        {work_unit_size.gpus}")
    logger.info(f"    memory (MB): {work_unit_size.mem}")
    logger.info(f"    time (mins): {work_unit_size.time}")
    logger.info( "--- per job array task ---")
    logger.info(f"    CPUs:        {array_task_size.cpus}")
    logger.info(f"    GPUs:        {array_task_size.gpus}")
    logger.info(f"    memory (MB): {array_task_size.mem}")
    logger.info(f"    time (mins): {array_task_size.time}")
    logger.info(f"--- {work_unit_count} total work units ---")
    logger.info(f"--- {math.ceil(work_unit_count/array_task_count)} work units per array task ---")
    logger.info(f"--- {array_task_count} job array tasks ---")

    # Error out if the resource request was too big
    if (array_task_count > max_array_tasks or args.work_unit_size.cpus > max_array_task_size.cpus
                                                or args.work_unit_size.gpus > max_array_task_size.gpus
                                                or args.work_unit_size.mem  > max_array_task_size.mem
                                                or args.work_unit_size.time > max_array_task_size.time):
        logger.error(f"Array task is too large for submission; {max_array_tasks} tasks of size " +
                     f"{max_array_task_size} can't fit {work_unit_count} work units of size {work_unit_size}")
        sys.exit(1)

    # Prepare job submission command
    arg_file_subdir = tempfile.mkdtemp(dir=arg_file_dir) if not args.dry_run else 'arg-file-subdir'
    batch_script = os.path.join(os.path.dirname(os.path.realpath(__file__)), "task.sh")
    work_unit_script = os.path.join(os.path.dirname(os.path.realpath(__file__)), "work_unit.py")
    ntasks = math.ceil(array_task_size.cpus/work_unit_size.cpus)
    sbatch_args += [f"--output={args.logfile}",
                    f"--open-mode={args.open_mode}",
                    f"--ntasks={ntasks}",
                    f"--cpus-per-task={work_unit_size.cpus}",
                    f"--mem-per-cpu={math.ceil(work_unit_size.mem/work_unit_size.cpus)}",
                    f"--time={array_task_size.time}",
                    f"--array=1-{array_task_count}"]
    if work_unit_size.gpus: sbatch_args.append(f"--gpus-per-task={work_unit_size.gpus}")
    submit_command = ["sbatch"] + sbatch_args \
                   + [batch_script, arg_file_subdir, delimiter_literal, work_unit_script, args.output, args.error,
                      "a" if args.open_mode == "append" else "w"] \
                   + command

    # Clean up any old argument file directories that don't have an associated job
    if not args.dry_run:
        current_job_ids = subprocess.check_output(["squeue", "-o", "%F"]).decode("utf-8").strip().split("\n")
        for subdir in next(os.walk(arg_file_dir))[1]:
            job_dir = os.path.join(arg_file_dir, subdir)
            # Don't delete anything recent to (probably) avoid a race condition
            if time.time() - os.path.getmtime(job_dir) < 30 * 24 * 60**2: continue # only delete if over a month old
            # Read the job ID from the file
            job_id_file = os.path.join(job_dir, "jobid.txt")
            other_job_id = open(job_id_file).read() if os.path.isfile(job_id_file) else ""
            # Delete the directory if the job ID file didn't exist or if the other job ID isn't shown by squeue
            if not other_job_id in current_job_ids:
                logger.info(f"Removing stale argument file directory {job_dir} (job {other_job_id})")
                shutil.rmtree(job_dir)

    # Set a couple of environment variables
    os.environ["OMP_NUM_THREADS"] = str(work_unit_size.cpus)

    # If this is a dry run, print what would have happened and exit here
    if args.dry_run:
        print("Job that would have been submitted: " + " ".join(submit_command))
        sys.exit()

    # Write arguments to files for each job array task
    os.makedirs(arg_file_dir, exist_ok=True)
    for i in range(array_task_count):
        with open(os.path.join(arg_file_subdir, f"{i+1}.in"), "w") as f:
            this_work_unit_args = work_unit_args[i::array_task_count]
            this_work_unit_indices = range(work_unit_count)[i::array_task_count]
            f.write(delimiter_literal.join(f"{i+1} {arg}" for arg, i in zip(this_work_unit_args,
                                                                            this_work_unit_indices)))

    # Submit the job
    try:
        logger.info("Submission command: " + " ".join(submit_command))
        job_submit_output = subprocess.check_output(submit_command).decode("utf-8").strip()
        #print(job_submit_output, file=sys.stderr)
        job_id = shlex.split(job_submit_output)[-1]
        with open(os.path.join(arg_file_subdir, "jobid.txt"), "w") as f:
            f.write(job_id)
        print(job_submit_output)
    except subprocess.CalledProcessError as e:
        logger.error("sbatch failed to properly submit job")
        logger.error(e.output.decode().strip())
        sys.exit(e.returncode)




if __name__ == "__main__":
    main()



# TODO
# - Put a job ID at the top of argument files so slurm-auto-array can automatically clean up any that didn't get cleaned by the jobs.
